# Lakehouse-to-RAG Pipeline

A complete end-to-end data pipeline that scrapes websites, processes data through a lakehouse architecture, and provides RAG (Retrieval-Augmented Generation) capabilities.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraper   â”‚â”€â”€â”€â–¶â”‚   MinIO     â”‚â”€â”€â”€â–¶â”‚   Spark     â”‚â”€â”€â”€â–¶â”‚   Delta     â”‚
â”‚             â”‚    â”‚  (Storage)  â”‚    â”‚  (ETL)      â”‚    â”‚   Lake      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   RAG API   â”‚â—€â”€â”€â”€â”‚   Chroma    â”‚â—€â”€â”€â”€â”‚ Embeddings  â”‚â—€â”€â”€â”€â”€â”€â”€â”˜
â”‚  (FastAPI)  â”‚    â”‚ (Vector DB) â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

- **Web Scraping**: Robust scraper with crawling capabilities
- **Data Lake**: MinIO-based storage with bronze/silver/gold layers
- **ETL Pipeline**: Spark-powered transformations with Delta Lake
- **Quality Monitoring**: Comprehensive data quality checks
- **Vector Search**: Chroma-based embeddings and retrieval
- **RAG API**: FastAPI service for question answering
- **Orchestration**: Airflow DAGs for pipeline automation
- **Monitoring**: Detailed logging and statistics

## ğŸ“ Project Structure

```
lakehouse-to-rag/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ docker-compose.yaml       # Service orchestration
â”œâ”€â”€ Makefile                  # Development commands
â”œâ”€â”€ architecture.png          # System architecture diagram
â”‚
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ scraper/             # Web scraping module
â”‚   â”‚   â”œâ”€â”€ scraper.py       # Main scraper class
â”‚   â”‚   â”œâ”€â”€ minio_utils.py   # MinIO integration
â”‚   â”‚   â”œâ”€â”€ __main__.py      # CLI interface
â”‚   â”‚   â”œâ”€â”€ requirements.txt # Dependencies
â”‚   â”‚   â””â”€â”€ Dockerfile       # Container definition
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                 # FastAPI service
â”‚   â”‚   â”œâ”€â”€ main.py          # API endpoints
â”‚   â”‚   â”œâ”€â”€ requirements.txt # Dependencies
â”‚   â”‚   â””â”€â”€ Dockerfile       # Container definition
â”‚   â”‚
â”‚   â”œâ”€â”€ helpers/             # Shared utilities
â”‚   â”‚   â”œâ”€â”€ duckdb_queries.py # Query utilities
â”‚   â”‚   â”œâ”€â”€ delta_queries.py  # Delta table queries
â”‚   â”‚   â””â”€â”€ requirements.txt  # Dependencies
â”‚   â”‚
â”‚   â””â”€â”€ tests/               # Test suite
â”‚       â”œâ”€â”€ test_scraper.py  # Scraper tests
â”‚       â”œâ”€â”€ test_etl.py      # ETL tests
â”‚       â””â”€â”€ test_api.py      # API tests
â”‚
â”œâ”€â”€ airflow/                  # Airflow configuration
â”‚   â””â”€â”€ dags/                # Pipeline DAGs
â”‚       â”œâ”€â”€ scrape_etl_dag.py # Main ETL pipeline
â”‚       â”œâ”€â”€ etl_utils.py      # ETL utilities
â”‚       â””â”€â”€ requirements.txt  # DAG dependencies
â”‚
â”œâ”€â”€ config/                   # Configuration files
â”‚   â”œâ”€â”€ scraper/             # Scraper configurations
â”‚   â”‚   â””â”€â”€ example.yaml     # Example scraper config
â”‚   â”œâ”€â”€ etl/                 # ETL configurations
â”‚   â””â”€â”€ api/                 # API configurations
â”‚
â”œâ”€â”€ data/                     # Data storage
â”‚   â”œâ”€â”€ delta/               # Delta Lake tables
â”‚   â”‚   â”œâ”€â”€ bronze/          # Raw data
â”‚   â”‚   â”œâ”€â”€ silver/          # Cleaned data
â”‚   â”‚   â””â”€â”€ gold/            # Final data
â”‚   â””â”€â”€ lineage/             # Data lineage logs
â”‚
â””â”€â”€ tests/                    # Integration tests
    â”œâ”€â”€ test_pipeline.py     # End-to-end tests
    â””â”€â”€ test_integration.py  # Service integration tests
```

## ğŸ› ï¸ Setup

### Prerequisites

- Docker and Docker Compose
- Python 3.10+
- 4GB+ RAM available

### Quick Start

1. **Clone and setup**:
   ```bash
   git clone <repository-url>
   cd lakehouse-to-rag
   ```

2. **Start services**:
   ```bash
   docker compose up -d
   ```

3. **Access services**:
   - Airflow UI: http://localhost:8080 (admin/admin)
   - MinIO Console: http://localhost:9001 (minioadmin/minioadmin)
   - FastAPI: http://localhost:8001
   - Chroma: http://localhost:8000

### Development Setup

1. **Create virtual environment**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Linux/Mac
   # or
   .venv\Scripts\activate     # Windows
   ```

2. **Install dependencies**:
   ```bash
   pip install -r src/scraper/requirements.txt
   pip install -r src/api/requirements.txt
   pip install -r src/helpers/requirements.txt
   ```

3. **Run tests**:
   ```bash
   make test
   ```

## ğŸ“– Usage

### Web Scraping

**Using CLI**:
```bash
# Single page scraping
python -m scraper --url https://example.com \
  --selectors '{"title": "h1", "content": ".main"}'

# Site crawling
python -m scraper --config config/scraper/example.yaml \
  --crawl --max-pages 50 --stats
```

**Using Airflow**:
1. Set Airflow Variables for configuration
2. Trigger the `lakehouse_etl_pipeline` DAG
3. Monitor progress in Airflow UI

### Data Pipeline

The ETL pipeline processes data through three stages:

1. **Bronze**: Raw scraped data with basic cleaning
2. **Silver**: Cleaned and normalized data
3. **Gold**: Deduplicated and enriched data

### RAG API

**Query the knowledge base**:
```bash
curl -X POST "http://localhost:8001/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic?"}'
```

## ğŸ”§ Configuration

### Scraper Configuration

The project includes comprehensive configuration examples in `sample_config/`:

**Basic Example** (`sample_config/example.yaml`):
```yaml
site_url: "https://example.com"
selectors:
  title: "h1, .title, .page-title"
  content: ".content, .main-content, .article-content"
  author: ".author, .byline"
  date: ".date, .published-date"

advanced:
  rate_limit: 1.0
  timeout: 30
  max_retries: 3
  min_content_length: 100
  respect_robots: true
  max_pages: 50
```

**Comprehensive Examples** (`sample_config/config.yaml`):
- Blog/News sites
- Documentation sites
- E-commerce pages
- Academic papers
- API documentation
- Forum/Community sites
- Portfolio sites

**Usage Examples**:
```bash
# Use basic example (Project Gutenberg)
python -m scraper --config sample_config/example.yaml --crawl

# List available examples
python -m scraper --config sample_config/config.yaml --list-examples

# Use specific Project Gutenberg example
python -m scraper --config sample_config/config.yaml --example gutenberg_classics --crawl

# Use catalog browsing
python -m scraper --config sample_config/config.yaml --example gutenberg_catalog --crawl --max-pages 10

# Use author pages
python -m scraper --config sample_config/config.yaml --example gutenberg_authors --crawl

# Create custom configuration
cp sample_config/example.yaml my_gutenberg_config.yaml
# Edit my_gutenberg_config.yaml with your preferences
python -m scraper --config my_gutenberg_config.yaml --crawl
```

**Test the Configuration**:
```bash
# Test the Project Gutenberg configuration
python src/scraper/test_config.py
```

### Airflow Variables

Set these in Airflow UI â†’ Admin â†’ Variables:
- `scraper_site_url`: Target website URL
- `scraper_selectors`: YAML string of CSS selectors
- `scraper_max_pages`: Maximum pages to crawl
- `scraper_crawl`: Enable/disable crawling

## ğŸ§ª Testing

### Unit Tests
```bash
# Test scraper
python -m pytest src/tests/test_scraper.py

# Test ETL
python -m pytest src/tests/test_etl.py

# Test API
python -m pytest src/tests/test_api.py
```

### Integration Tests
```bash
# End-to-end pipeline test
python -m pytest tests/test_pipeline.py

# Service integration test
python -m pytest tests/test_integration.py
```

## ğŸ“Š Monitoring

### Data Quality

The pipeline includes comprehensive data quality checks:
- Record counts at each stage
- Content length analysis
- Missing value detection
- Duplicate identification

### Logging

- **Scraper**: Progress and error logs
- **ETL**: Transformation statistics
- **API**: Request/response logs
- **Airflow**: Task execution logs

## ğŸš€ Deployment

### Production Setup

1. **Environment Variables**:
   ```bash
   export MINIO_ENDPOINT=your-minio-endpoint
   export MINIO_ACCESS_KEY=your-access-key
   export MINIO_SECRET_KEY=your-secret-key
   ```

2. **Docker Compose**:
   ```bash
   docker compose -f docker-compose.prod.yaml up -d
   ```

3. **Monitoring**:
   - Set up Prometheus/Grafana for metrics
   - Configure alerting for pipeline failures
   - Monitor resource usage

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

### Development Guidelines

- Follow PEP 8 style guide
- Add type hints to all functions
- Include docstrings for all classes and methods
- Write tests for new features
- Update documentation as needed

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

- **Issues**: Create an issue on GitHub
- **Documentation**: Check the docs/ directory
- **Community**: Join our discussions

---

**Built with â¤ï¸ for the data community** 